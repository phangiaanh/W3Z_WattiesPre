# @package _global_.training.optimizer

type: AdamW
lr: 0.0001
weight_decay: 0.01
betas: [0.9, 0.999]
eps: 1e-8

